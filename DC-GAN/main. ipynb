import os
import numpy as np
from PIL import Image
from tqdm import tqdm 

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from torchvision.utils import save_image
from torch.autograd import Variable

# Create a directory for saving generated images
os.makedirs("images", exist_ok=True)

# Setting hyperparameters
n_epochs = 400
batch_size = 5  # Reduce batch size for larger image size
lr = 0.00005
b1 = 0.5
b2 = 0.999
n_cpu = 8
latent_dim = 128
img_size = 640  # Set to 640 for final size of 640x640
channels = 3   # Set to 3 for color images (RGB)
sample_interval = 400

print("n_epochs:", n_epochs)
print("batch_size:", batch_size)
print("lr:", lr)
print("b1:", b1)
print("b2:", b2)
print("n_cpu:", n_cpu)
print("latent_dim:", latent_dim)
print("img_size:", img_size)
print("channels:", channels)
print("sample_interval:", sample_interval)

cuda = True if torch.cuda.is_available() else False

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        self.init_size = img_size // 4
        self.l1 = nn.Linear(latent_dim, 128 * self.init_size ** 2)

        self.conv_blocks = nn.Sequential(
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 64, 3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(64, channels, 3, stride=1, padding=1),
            nn.Tanh(),
        )

        self.apply(weights_init_normal)  # Initialize weights

    def forward(self, z):
        out = self.l1(z)
        out = out.view(out.shape[0], 128, self.init_size, self.init_size)
        img = self.conv_blocks(out)
        return img


class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        def discriminator_block(in_filters, out_filters, bn=True):
            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]
            if bn:
                block.append(nn.BatchNorm2d(out_filters, 0.8))
            return block

        self.model = nn.Sequential(
            *discriminator_block(channels, 32, bn=False),
            *discriminator_block(32, 64),
            *discriminator_block(64, 128),
            *discriminator_block(128, 256),
        )

        # The height and width of downsampled image
        ds_size = img_size // 2 ** 4
        self.adv_layer = nn.Sequential(nn.Linear(256 * ds_size ** 2, 1), nn.Sigmoid())

    def forward(self, img):
        out = self.model(img)
        out = out.view(out.shape[0], -1)
        validity = self.adv_layer(out)
        return validity

# Loss function
adversarial_loss = torch.nn.BCELoss()

# Initialize generator and discriminator
generator = Generator()
discriminator = Discriminator()

if cuda:
    generator.cuda()
    discriminator.cuda()
    adversarial_loss.cuda()

# Initialize weights
generator.apply(weights_init_normal)
discriminator.apply(weights_init_normal)

# Custom dataset class for PKLot
class PKLotDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        for subdir, _, files in os.walk(root_dir):
            for file in files:
                if file.endswith(('png', 'jpg', 'jpeg')):
                    self.image_paths.append(os.path.join(subdir, file))

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')  # Convert to RGB for color images
        if self.transform:
            image = self.transform(image)
        return image

# Configure data loader
dataloader = torch.utils.data.DataLoader(
    PKLotDataset(
        root_dir='/kaggle/input/pklot-training-2/train',  # Updated path to the extracted PKLot dataset
        transform=transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
        ]),
    ),
    batch_size=batch_size,
    shuffle=True,
)

# Optimizers
optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))

# Tensor type for GPU if available
Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor

# ----------
#  Training
# ----------

# Revised Training Loop with tqdm for progress bar
accumulation_steps = 4  # Accumulate gradients over 4 batches

for epoch in range(n_epochs):
    # Initialize tqdm for progress bar
    data_iterator = tqdm(enumerate(dataloader),
                         total=len(dataloader),
                         desc=f'Epoch {epoch}/{n_epochs}',
                         dynamic_ncols=True)
    
    for i, imgs in data_iterator:
        # Adversarial ground truths
        smooth_real = Variable(Tensor(imgs.size(0), 1).uniform_(0.9, 1.0), requires_grad=False)
        smooth_fake = Variable(Tensor(imgs.size(0), 1).uniform_(0.0, 0.1), requires_grad=False)

        # Configure input
        real_imgs = Variable(imgs.type(Tensor))

        # -----------------
        #  Train Generator
        # -----------------
        optimizer_G.zero_grad()

        # Sample noise as generator input
        z = Variable(Tensor(np.random.normal(0, 1, (imgs.size(0), latent_dim))))

        # Generate a batch of images
        gen_imgs = generator(z)

        # Loss measures generator's ability to fool the discriminator
        g_loss = adversarial_loss(discriminator(gen_imgs), smooth_real)

        g_loss.backward()

        # Accumulate gradients for several batches
        if (i+1) % accumulation_steps == 0:
            optimizer_G.step()
            optimizer_G.zero_grad()

        # ---------------------
        #  Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()

        # Measure discriminator's ability to classify real from generated samples
        real_loss = adversarial_loss(discriminator(real_imgs), smooth_real)
        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), smooth_fake)
        d_loss = (real_loss + fake_loss) / 2

        d_loss.backward()

        # Accumulate gradients for several batches
        if (i+1) % accumulation_steps == 0:
            optimizer_D.step()
            optimizer_D.zero_grad()

        # Update tqdm description to show current losses
        data_iterator.set_postfix(D_loss=d_loss.item(), G_loss=g_loss.item())

        # Save generated images at sample_interval
        batches_done = epoch * len(dataloader) + i
        if batches_done % sample_interval == 0:
            save_image(gen_imgs.data[:1], "images/%d.png" % batches_done, nrow=1, normalize=True)
